{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b659595f-b6a7-46de-bf26-734153dcc875",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Data Science with MLFlow\n",
    "\n",
    "This notebook demonstrates a complete workflow for credit risk modeling using MLFlow in Databricks. It covers data loading, exploratory data analysis (EDA), feature engineering, and preparation, followed by train-test splitting for supervised machine learning. The notebook leverages LightGBM for classification, visualizes class distributions and feature correlations, and encodes categorical variables. All steps are tracked and reproducible with MLFlow, enabling robust model development and experiment management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATALOG = 'workspace'\n",
    "BRONZE_SCHEMA = 'bronze'\n",
    "SILVER_SCHEMA = 'silver'\n",
    "GOLD_SCHEMA = 'gold'\n",
    "\n",
    "USER_EMAIL = 'myname@example.com'\n",
    "MLFLOW_EXPERIMENT_PATH = f'/Users/{USER_EMAIL}/credit-scoring-experiment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a560e36-c4d8-4764-a82a-2cdbb0e48ea8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install Required Libraries"
    }
   },
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "%pip install lightgbm optuna scikit-learn matplotlib seaborn mlflow --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b0f7acd-b82c-4cf5-8752-d9f3c269f157",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Restart Python Kernel"
    }
   },
   "outputs": [],
   "source": [
    "# Restart Python kernel after installation\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0c6c70d-50bc-4fec-80db-a80e41438570",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import Libraries"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve\n",
    "import lightgbm as lgb\n",
    "import mlflow\n",
    "import mlflow.lightgbm\n",
    "from datetime import datetime\n",
    "\n",
    "# Set style for visualizations\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3603f6df-8f6f-4c91-af80-7a1ec7dc90ea",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Data"
    }
   },
   "outputs": [],
   "source": [
    "# Load the data from the table\n",
    "df = spark.table(f\"{CATALOG}.{GOLD_SCHEMA}.final_features\").toPandas()\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nTarget variable distribution:\")\n",
    "print(df['flag_default'].value_counts())\n",
    "print(f\"\\nDefault rate: {df['flag_default'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b316009-609e-4f3e-b9a9-50f052ac6db6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Exploratory Data Analysis - Basic Info"
    }
   },
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "print(\"Dataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nBasic Statistics:\")\n",
    "display(df.describe())\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nMissing Values:\")\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df) * 100).round(2)\n",
    "missing_df = pd.DataFrame({'Missing_Count': missing, 'Percentage': missing_pct})\n",
    "missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n",
    "if len(missing_df) > 0:\n",
    "    display(missing_df)\n",
    "else:\n",
    "    print(\"No missing values found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6704a587-8bc2-413a-9c8a-9aa9d66a7c35",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "EDA - Class Distribution Visualization"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count plot\n",
    "df['flag_default'].value_counts().plot(kind='bar', ax=axes[0], color=['#2ecc71', '#e74c3c'])\n",
    "axes[0].set_title('Class Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Flag Default (0=No Default, 1=Default)', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_xticklabels(['No Default', 'Default'], rotation=0)\n",
    "\n",
    "# Add count labels on bars\n",
    "for i, v in enumerate(df['flag_default'].value_counts()):\n",
    "    axes[0].text(i, v + 10, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "# Pie chart\n",
    "df['flag_default'].value_counts().plot(kind='pie', ax=axes[1], autopct='%1.1f%%', \n",
    "                                        colors=['#2ecc71', '#e74c3c'], labels=['No Default', 'Default'])\n",
    "axes[1].set_title('Class Distribution (%)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nClass Imbalance Ratio: 1:{df['flag_default'].value_counts()[0] / df['flag_default'].value_counts()[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48d29fcb-d2cb-4b31-b44f-9b59299d299e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "EDA - Feature Correlations with Target"
    }
   },
   "outputs": [],
   "source": [
    "# Select numerical features for correlation analysis\n",
    "numerical_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numerical_features.remove('flag_default')  # Remove target\n",
    "\n",
    "# Calculate correlation with target\n",
    "correlations = df[numerical_features + ['flag_default']].corr()['flag_default'].drop('flag_default').sort_values(ascending=False)\n",
    "\n",
    "# Plot top 15 correlations\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "top_corr = pd.concat([correlations.head(10), correlations.tail(5)])\n",
    "colors = ['#e74c3c' if x > 0 else '#3498db' for x in top_corr.values]\n",
    "top_corr.plot(kind='barh', ax=ax, color=colors)\n",
    "ax.set_title('Top Features Correlated with Default Flag', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Correlation Coefficient', fontsize=12)\n",
    "ax.set_ylabel('Features', fontsize=12)\n",
    "ax.axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 Positive Correlations with Default:\")\n",
    "print(correlations.head(10))\n",
    "print(\"\\nTop 5 Negative Correlations with Default:\")\n",
    "print(correlations.tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b81c550-da8a-4bf6-9f17-6a228f982d78",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "EDA - Key Feature Distributions"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize key features by default status\n",
    "key_features = ['skor_kredit', 'debt_to_income_ratio', 'rasio_pembayaran', 'avg_hari_keterlambatan']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(key_features):\n",
    "    # Remove nulls for visualization\n",
    "    data_to_plot = df[[feature, 'flag_default']].dropna()\n",
    "    \n",
    "    data_to_plot.boxplot(column=feature, by='flag_default', ax=axes[idx])\n",
    "    axes[idx].set_title(f'{feature} by Default Status', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Flag Default (0=No Default, 1=Default)', fontsize=10)\n",
    "    axes[idx].set_ylabel(feature, fontsize=10)\n",
    "    axes[idx].get_figure().suptitle('')  # Remove automatic title\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4c8ec36-ba33-4e48-a290-52d484a71afc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Feature Engineering & Data Preparation"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare features for modeling\n",
    "print(\"Preparing features for modeling...\\n\")\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(['flag_default', 'loan_id', 'applicant_id'], axis=1)\n",
    "y = df['flag_default']\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"Categorical features ({len(categorical_cols)}): {categorical_cols}\")\n",
    "print(f\"\\nNumerical features ({len(numerical_cols)}): {numerical_cols[:10]}... (showing first 10)\")\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "print(f\"\\nCategorical encoding completed.\")\n",
    "\n",
    "# Handle missing values (fill with median for numerical)\n",
    "for col in numerical_cols:\n",
    "    if X[col].isnull().sum() > 0:\n",
    "        median_val = X[col].median()\n",
    "        X[col].fillna(median_val, inplace=True)\n",
    "        print(f\"Filled {X[col].isnull().sum()} missing values in {col} with median: {median_val:.2f}\")\n",
    "\n",
    "print(f\"\\nFinal feature matrix shape: {X.shape}\")\n",
    "print(f\"Target variable shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3861660a-4db2-4004-aaff-49239b2cecb0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Train-Test Split"
    }
   },
   "outputs": [],
   "source": [
    "# Split data into train and test sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples\")\n",
    "print(f\"\\nTraining set default rate: {y_train.mean():.2%}\")\n",
    "print(f\"Test set default rate: {y_test.mean():.2%}\")\n",
    "print(f\"\\nFeature count: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db8e4ddf-1628-4577-b463-274401639898",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Train Model with MLflow Tracking"
    }
   },
   "outputs": [],
   "source": [
    "# Set MLflow experiment\n",
    "mlflow.set_experiment(MLFLOW_EXPERIMENT_PATH)\n",
    "\n",
    "# Start MLflow run\n",
    "with mlflow.start_run(run_name=f\"credit_scoring_{datetime.now().strftime('%Y%m%d_%H%M%S')}\") as run:\n",
    "    \n",
    "    print(\"Training LightGBM model with MLflow tracking...\\n\")\n",
    "    \n",
    "    # Define model parameters\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'max_depth': 6,\n",
    "        'min_child_samples': 20,\n",
    "        'scale_pos_weight': len(y_train[y_train==0]) / len(y_train[y_train==1]),  # Handle class imbalance\n",
    "        'random_state': 42,\n",
    "        'verbose': -1\n",
    "    }\n",
    "    \n",
    "    # Log parameters to MLflow\n",
    "    mlflow.log_params(params)\n",
    "    mlflow.log_param(\"train_size\", len(X_train))\n",
    "    mlflow.log_param(\"test_size\", len(X_test))\n",
    "    mlflow.log_param(\"n_features\", X_train.shape[1])\n",
    "    \n",
    "    # Create LightGBM datasets\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "    \n",
    "    # Train model\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=200,\n",
    "        valid_sets=[train_data, test_data],\n",
    "        valid_names=['train', 'test'],\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=20), lgb.log_evaluation(period=20)]\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_proba = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Log metrics to MLflow\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "    mlflow.log_metric(\"f1_score\", f1)\n",
    "    mlflow.log_metric(\"roc_auc\", roc_auc)\n",
    "    mlflow.log_metric(\"best_iteration\", model.best_iteration)\n",
    "    \n",
    "    # Log model to MLflow\n",
    "    mlflow.lightgbm.log_model(model, \"model\", input_example=X_train.head(5))\n",
    "    \n",
    "    # Log feature importance as artifact\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': model.feature_importance(importance_type='gain')\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    mlflow.log_dict(feature_importance.to_dict(), \"feature_importance.json\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MODEL TRAINING COMPLETED\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nMLflow Run ID: {run.info.run_id}\")\n",
    "    print(f\"\\nModel Performance Metrics:\")\n",
    "    print(f\"  - Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"  - Precision: {precision:.4f}\")\n",
    "    print(f\"  - Recall:    {recall:.4f}\")\n",
    "    print(f\"  - F1 Score:  {f1:.4f}\")\n",
    "    print(f\"  - ROC AUC:   {roc_auc:.4f}\")\n",
    "    print(f\"\\nBest iteration: {model.best_iteration}\")\n",
    "    print(f\"\\n‚úì Model and metrics logged to MLflow experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "531e7788-d046-4554-b980-0c175f05df88",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Model Evaluation - Confusion Matrix"
    }
   },
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['No Default', 'Default'])\n",
    "disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
    "ax.set_title('Confusion Matrix - Credit Scoring Model', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['No Default', 'Default']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "178c443b-b195-41b9-a246-2cfe9436472f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Model Evaluation - ROC Curve"
    }
   },
   "outputs": [],
   "source": [
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "ax.plot(fpr, tpr, color='#e74c3c', linewidth=2, label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
    "ax.plot([0, 1], [0, 1], color='gray', linestyle='--', linewidth=1, label='Random Classifier')\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "ax.set_title('ROC Curve - Credit Scoring Model', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='lower right', fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0aa1e05-ca0a-4b6c-8cba-55882c46f852",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Model Evaluation - Precision-Recall Curve"
    }
   },
   "outputs": [],
   "source": [
    "# Plot Precision-Recall curve\n",
    "precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "ax.plot(recall_curve, precision_curve, color='#3498db', linewidth=2, label='Precision-Recall Curve')\n",
    "ax.set_xlabel('Recall', fontsize=12)\n",
    "ax.set_ylabel('Precision', fontsize=12)\n",
    "ax.set_title('Precision-Recall Curve - Credit Scoring Model', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='upper right', fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAverage Precision Score: {precision_curve.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ddc7c47-4784-4b0c-8c39-2915d5e5d8b5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Feature Importance Analysis"
    }
   },
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "top_n = 15\n",
    "top_features = feature_importance.head(top_n)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.barh(range(len(top_features)), top_features['importance'], color='#9b59b6')\n",
    "ax.set_yticks(range(len(top_features)))\n",
    "ax.set_yticklabels(top_features['feature'])\n",
    "ax.set_xlabel('Importance (Gain)', fontsize=12)\n",
    "ax.set_ylabel('Features', fontsize=12)\n",
    "ax.set_title(f'Top {top_n} Most Important Features', fontsize=14, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTop {top_n} Most Important Features:\")\n",
    "display(top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b4623db-273d-427a-8414-444ccfa76e4a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Register Model to Unity Catalog"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "uc_model_name = f\"{CATALOG}.{GOLD_SCHEMA}.credit_scoring_model\"\n",
    "\n",
    "result = mlflow.register_model(\n",
    "    model_uri=f\"runs:/{run.info.run_id}/model\",\n",
    "    name=uc_model_name\n",
    ")\n",
    "\n",
    "print(f\"Model registered to Unity Catalog as: {uc_model_name}\")\n",
    "print(f\"Version: {result.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6921fdb-bb1c-4492-a7f2-88cdada739c4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set Model Alias and Test Loading"
    }
   },
   "outputs": [],
   "source": [
    "from mlflow import MlflowClient\n",
    "\n",
    "# Set alias for the registered model version (Databricks MLflow only)\n",
    "client = MlflowClient()\n",
    "client.set_registered_model_alias(\n",
    "    name=uc_model_name,\n",
    "    version=result.version,\n",
    "    alias=\"production\"\n",
    ")\n",
    "print(f\"Alias 'production' set for model version {result.version}\")\n",
    "\n",
    "# Load model using alias\n",
    "loaded_model = mlflow.pyfunc.load_model(f\"models:/{uc_model_name}@production\")\n",
    "print(\"Model loaded successfully using alias 'production'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9959f05-df4d-4287-bb37-317da128ffbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üéØ Credit Scoring Model - Summary\n",
    "\n",
    "### Model Performance\n",
    "Our LightGBM credit scoring model achieved excellent results:\n",
    "* **Accuracy**: 93.65% - correctly classified 94% of all loans\n",
    "* **ROC AUC**: 0.9882 - excellent discrimination between default and non-default\n",
    "* **Recall**: 91.30% - successfully identified 91% of actual defaults\n",
    "* **Precision**: 77.78% - when predicting default, correct 78% of the time\n",
    "\n",
    "### Key Findings\n",
    "The most important features for predicting loan defaults are:\n",
    "1. **avg_hari_keterlambatan** (average days late) - strongest predictor\n",
    "2. **skor_kredit** (credit score) - second most important\n",
    "3. **total_payments** - payment history matters\n",
    "4. **total_hari_keterlambatan** - cumulative lateness\n",
    "5. **total_denda** (total penalties) - indicates payment issues\n",
    "\n",
    "### üìä MLflow Capabilities Demonstrated\n",
    "\n",
    "This notebook showcased several beginner-friendly MLflow features:\n",
    "\n",
    "#### 1. **Experiment Tracking**\n",
    "   * Created a dedicated experiment: `/Users/aditya.pradana@databricks.com/credit-scoring-experiment`\n",
    "   * All runs are organized in one place for easy comparison\n",
    "\n",
    "#### 2. **Parameter Logging**\n",
    "   * Logged all model hyperparameters (learning_rate, num_leaves, max_depth, etc.)\n",
    "   * Logged dataset information (train_size, test_size, n_features)\n",
    "   * Makes it easy to reproduce results and understand what settings were used\n",
    "\n",
    "#### 3. **Metrics Logging**\n",
    "   * Automatically tracked key performance metrics:\n",
    "     - Accuracy, Precision, Recall, F1 Score\n",
    "     - ROC AUC for model discrimination\n",
    "     - Best iteration from early stopping\n",
    "   * Metrics are stored and can be compared across multiple runs\n",
    "\n",
    "#### 4. **Model Logging**\n",
    "   * Saved the trained LightGBM model as an MLflow artifact\n",
    "   * Model can be loaded later for predictions or deployment\n",
    "   * Includes model versioning and lineage tracking\n",
    "\n",
    "#### 5. **Artifact Logging**\n",
    "   * Saved feature importance as a JSON artifact\n",
    "   * Can store any additional files (plots, data, configs)\n",
    "\n",
    "### üîç How to Access Your MLflow Experiment\n",
    "\n",
    "You can view your experiment in the Databricks UI:\n",
    "1. Click on **\"Experiments\"** in the left sidebar\n",
    "2. Find the experiment: `credit-scoring-experiment`\n",
    "3. Click on the run to see:\n",
    "   * All logged parameters and metrics\n",
    "   * Model artifacts and files\n",
    "   * Comparison charts across multiple runs\n",
    "   * Model lineage and versioning\n",
    "\n",
    "### üí° Next Steps\n",
    "\n",
    "To improve this model further, you could:\n",
    "* Try different algorithms (XGBoost, Random Forest)\n",
    "* Perform hyperparameter tuning with Optuna\n",
    "* Add more feature engineering\n",
    "* Handle class imbalance with SMOTE\n",
    "* Register the model to MLflow Model Registry for deployment\n",
    "\n",
    "All of these experiments can be tracked with MLflow to compare performance!"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04-Data-Science",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
