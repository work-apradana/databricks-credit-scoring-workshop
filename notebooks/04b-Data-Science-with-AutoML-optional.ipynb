{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "441d8fd6-4b9b-4426-92b1-a0a3d9ccfe90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Credit Scoring Model with AutoML\n",
    "\n",
    "**Generated by:** Databricks Data Science Agent\n",
    "\n",
    "**Purpose:** Demonstrate AutoML capabilities for building a credit scoring model to predict loan defaults\n",
    "\n",
    "**Dataset:** `apradana_demo_permata.gold.final_features`\n",
    "\n",
    "**Target Variable:** `flag_default` (binary classification: 0 = no default, 1 = default)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2144939c-d8a7-46dc-a0f2-97e20444aa0f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "⚠️ Compute Requirement"
    }
   },
   "source": [
    "## ⚠️ Important: AutoML Compute Requirements\n",
    "\n",
    "**AutoML requires a classic cluster** (single-node or multi-node) and **cannot run on serverless compute**.\n",
    "\n",
    "### To run this notebook:\n",
    "\n",
    "1. **Switch to a classic cluster:**\n",
    "   * Click the compute selector in the top-right corner of this notebook\n",
    "   * Select an existing classic cluster, or\n",
    "   * Create a new cluster:\n",
    "     - Click \"Create compute\"\n",
    "     - Choose **Single Node** for a quick demo (cheaper and faster to start)\n",
    "     - Recommended: `i3.xlarge` or `m5d.large` instance type\n",
    "     - Runtime: DBR 13.3 LTS ML or higher\n",
    "\n",
    "2. **Why classic cluster?**\n",
    "   * AutoML needs to install additional libraries\n",
    "   * Requires persistent compute for experiment tracking\n",
    "   * Needs access to MLflow experiment management\n",
    "\n",
    "3. **Once connected to a classic cluster**, run all cells below\n",
    "\n",
    "---\n",
    "\n",
    "*Note: This notebook was generated by Databricks Data Science Agent*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "817fb7b8-0d82-48df-a7fb-524f22793300",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load and prepare data"
    }
   },
   "outputs": [],
   "source": [
    "# Load the credit data from Unity Catalog\n",
    "df = spark.table(\"apradana_demo_permata.gold.final_features\")\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Total records: {df.count()}\")\n",
    "print(f\"\\nTarget variable distribution:\")\n",
    "df.groupBy(\"flag_default\").count().orderBy(\"flag_default\").show()\n",
    "\n",
    "# Convert to Pandas for AutoML (AutoML works with Pandas DataFrames)\n",
    "df_pandas = df.toPandas()\n",
    "\n",
    "print(f\"\\nDataset shape: {df_pandas.shape}\")\n",
    "print(f\"Features available: {df_pandas.shape[1] - 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f12db2cf-19c0-495b-8877-3d2f7ed3d34a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install AutoML (for Serverless compute)"
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-automl-runtime --quiet\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6177a156-ef38-40d7-8d66-0b1968f5ee35",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Run AutoML classification experiment"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-6224717010893906>, line 4\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Install AutoML runtime (required on some cluster configurations)\u001B[39;00m\n",
       "\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdatabricks\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m automl\n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Run AutoML for binary classification\u001B[39;00m\n",
       "\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# AutoML will automatically:\u001B[39;00m\n",
       "\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# - Split data into train/validation/test sets\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# - Log experiments to MLflow\u001B[39;00m\n",
       "\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# - Generate a notebook with the best model\u001B[39;00m\n",
       "\u001B[1;32m     15\u001B[0m summary \u001B[38;5;241m=\u001B[39m automl\u001B[38;5;241m.\u001B[39mclassify(\n",
       "\u001B[1;32m     16\u001B[0m     dataset\u001B[38;5;241m=\u001B[39mdf_pandas,\n",
       "\u001B[1;32m     17\u001B[0m     target_col\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mflag_default\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m     20\u001B[0m     experiment_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/Users/aditya.pradana@databricks.com/credit_scoring_automl\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     21\u001B[0m )\n",
       "\n",
       "\u001B[0;31mImportError\u001B[0m: cannot import name 'automl' from 'databricks' (/databricks/python_shell/lib/databricks/__init__.py)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ImportError",
        "evalue": "cannot import name 'automl' from 'databricks' (/databricks/python_shell/lib/databricks/__init__.py)"
       },
       "metadata": {
        "errorSummary": ""
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": "NOTEBOOK_USER_ERROR",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "pysparkSummary": null,
        "sqlState": "KAN00",
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
        "File \u001B[0;32m<command-6224717010893906>, line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Install AutoML runtime (required on some cluster configurations)\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdatabricks\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m automl\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Run AutoML for binary classification\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# AutoML will automatically:\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# - Split data into train/validation/test sets\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# - Log experiments to MLflow\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# - Generate a notebook with the best model\u001B[39;00m\n\u001B[1;32m     15\u001B[0m summary \u001B[38;5;241m=\u001B[39m automl\u001B[38;5;241m.\u001B[39mclassify(\n\u001B[1;32m     16\u001B[0m     dataset\u001B[38;5;241m=\u001B[39mdf_pandas,\n\u001B[1;32m     17\u001B[0m     target_col\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mflag_default\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     20\u001B[0m     experiment_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/Users/aditya.pradana@databricks.com/credit_scoring_automl\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     21\u001B[0m )\n",
        "\u001B[0;31mImportError\u001B[0m: cannot import name 'automl' from 'databricks' (/databricks/python_shell/lib/databricks/__init__.py)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Install AutoML runtime (required on some cluster configurations)\n",
    "\n",
    "\n",
    "from databricks import automl\n",
    "\n",
    "# Run AutoML for binary classification\n",
    "# AutoML will automatically:\n",
    "# - Split data into train/validation/test sets\n",
    "# - Try multiple algorithms (logistic regression, decision trees, random forest, XGBoost, LightGBM, etc.)\n",
    "# - Perform hyperparameter tuning\n",
    "# - Handle feature engineering\n",
    "# - Log experiments to MLflow\n",
    "# - Generate a notebook with the best model\n",
    "\n",
    "summary = automl.classify(\n",
    "    dataset=df_pandas,\n",
    "    target_col=\"flag_default\",\n",
    "    primary_metric=\"f1\",  # Good for imbalanced classification\n",
    "    timeout_minutes=10,  # Limit runtime for demo purposes\n",
    "    experiment_name=\"/Users/aditya.pradana@databricks.com/credit_scoring_automl\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"AutoML Run Complete!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e666e89-e1cd-45d0-bba5-988ccdc48f31",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "View AutoML results and best model"
    }
   },
   "outputs": [],
   "source": [
    "# Display the best trial information\n",
    "print(f\"Best trial ID: {summary.best_trial_notebook_id}\")\n",
    "print(f\"Best model metrics:\")\n",
    "print(f\"  - F1 Score: {summary.best_trial_f1_score:.4f}\")\n",
    "print(f\"  - Precision: {summary.best_trial_precision_score:.4f}\")\n",
    "print(f\"  - Recall: {summary.best_trial_recall_score:.4f}\")\n",
    "print(f\"  - Accuracy: {summary.best_trial_accuracy_score:.4f}\")\n",
    "\n",
    "print(f\"\\nExperiment URL: {summary.experiment.experiment_url}\")\n",
    "print(f\"\\nBest trial notebook: {summary.best_trial_notebook_url}\")\n",
    "print(\"\\n\uD83D\uDCA1 Click the notebook URL above to see the detailed model code and evaluation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8951d27c-e8a6-473b-8af5-96f183fbe475",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## \uD83D\uDE80 AutoML Simplifies the ML Lifecycle\n",
    "\n",
    "### What AutoML Just Did For You:\n",
    "\n",
    "1. **Automated Algorithm Selection**\n",
    "   * Tested multiple algorithms (Logistic Regression, Random Forest, XGBoost, LightGBM, etc.)\n",
    "   * Compared performance across different model types\n",
    "\n",
    "2. **Hyperparameter Optimization**\n",
    "   * Automatically tuned parameters for each algorithm\n",
    "   * Used intelligent search strategies to find optimal configurations\n",
    "\n",
    "3. **Feature Engineering**\n",
    "   * Handled categorical variables (one-hot encoding)\n",
    "   * Managed missing values\n",
    "   * Scaled numerical features appropriately\n",
    "\n",
    "4. **Model Evaluation**\n",
    "   * Split data into train/validation/test sets\n",
    "   * Calculated multiple metrics (F1, precision, recall, accuracy, ROC-AUC)\n",
    "   * Generated confusion matrices and feature importance plots\n",
    "\n",
    "5. **MLflow Integration**\n",
    "   * Logged all experiments automatically\n",
    "   * Tracked parameters, metrics, and artifacts\n",
    "   * Registered the best model for deployment\n",
    "\n",
    "6. **Reproducible Notebooks**\n",
    "   * Generated editable notebooks for each trial\n",
    "   * Included complete code for the best model\n",
    "   * Easy to customize and retrain\n",
    "\n",
    "### Time Saved:\n",
    "* **Without AutoML**: 2-3 days of manual experimentation\n",
    "* **With AutoML**: 10 minutes of automated optimization\n",
    "\n",
    "### Next Steps:\n",
    "* Review the best model notebook for detailed insights\n",
    "* Register the model to Unity Catalog for deployment\n",
    "* Use the model for batch or real-time scoring"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04b-Data-Science-with-AutoML",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}