{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "In this section, we will perform data cleaning on the credit scoring dataset. This includes handling missing values, correcting data types, and removing duplicates. Data cleaning is a crucial step in the data preprocessing pipeline as it ensures that the data is accurate and ready for analysis and modeling.\n",
    "\n",
    "*Note: Adjust the catalog and schema parameter as needed based on your Databricks environment setup.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c981d01-425b-40b6-9635-647ea9175d47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG = 'workspace'\n",
    "BRONZE_SCHEMA = 'bronze'\n",
    "SILVER_SCHEMA = 'silver'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "905ba76a-6e38-4452-b3b1-6b9924ad1491",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the silver schema if it doesn't exist\n",
    "display(\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SILVER_SCHEMA}\n",
    "    \"\"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bf24522-224a-4db8-9391-5c5af578dbcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Clean the data in Bronze Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64b41e5f-c144-41ef-8e0f-0da2e3840593",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Clean Applicants Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b6f9212-8731-4413-b1ed-04dbd5337cac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clean applicants data\n",
    "\n",
    "applicants_df = spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.raw_applicants\")\n",
    "applicants_df.limit(5).display()\n",
    "\n",
    "# trim whitespace in nama_lengkap column\n",
    "from pyspark.sql.functions import trim\n",
    "\n",
    "applicants_df = applicants_df.withColumn(\"nama_lengkap\", trim(\"nama_lengkap\"))\n",
    "# applicants_df.limit(5).display()\n",
    "\n",
    "# lower case jenis_kelamin column\n",
    "from pyspark.sql.functions import lower\n",
    "\n",
    "applicants_df = applicants_df.withColumn(\"jenis_kelamin\", lower(\"jenis_kelamin\"))\n",
    "# applicants_df.limit(5).display()\n",
    "\n",
    "# In column jenis_kelamin, convert male, p, l, laki-laki, pria to m and perempuan, wanita, female to f\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "applicants_df = applicants_df.withColumn(\n",
    "    \"jenis_kelamin\",\n",
    "    when(col(\"jenis_kelamin\").isin(\"male\", \"p\", \"pria\", \"laki-laki\", \"l\"), \"m\")\n",
    "    .when(col(\"jenis_kelamin\").isin(\"perempuan\", \"female\", \"wanita\"), \"f\")\n",
    "    .otherwise(col(\"jenis_kelamin\"))\n",
    ")\n",
    "# applicants_df.limit(5).display()\n",
    "\n",
    "# convert pendapatan_bulanan to fully numerical column. Remove non-numeric string if needed\n",
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "# Remove non-numeric characters except dot and comma, then convert to float\n",
    "applicants_df = applicants_df.withColumn(\n",
    "    \"pendapatan_bulanan\",\n",
    "    regexp_replace(col(\"pendapatan_bulanan\"), \"[^0-9]\", \"\")\n",
    "    .cast(\"int\")\n",
    ")\n",
    "\n",
    "silver_applicants_df = applicants_df\n",
    "silver_applicants_df.limit(5).display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "514b9e8c-36a2-4d03-8f72-b97da25a87f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Clean Loans Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d227f89-48e2-4cf9-be83-0e404ab77a9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clean loans data\n",
    "\n",
    "loans_df = spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.raw_loans\")\n",
    "loans_df.limit(5).display()\n",
    "\n",
    "# trim whitespace in tujuan_pinjaman column\n",
    "from pyspark.sql.functions import trim\n",
    "\n",
    "loans_df = loans_df.withColumn(\"tujuan_pinjaman\", trim(\"tujuan_pinjaman\"))\n",
    "\n",
    "# lower case status_persetujuan column\n",
    "from pyspark.sql.functions import lower\n",
    "\n",
    "loans_df = loans_df.withColumn(\"status_persetujuan\", lower(\"status_persetujuan\"))\n",
    "# distinct_status_df = loans_df.select(\"status_persetujuan\").distinct()\n",
    "# display(distinct_status_df)\n",
    "\n",
    "# In column status_persetujuan, convert dsetujui to approved, ditolak to rejected, 'dalam proses' to pending\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "loans_df = loans_df.withColumn(\n",
    "    \"status_persetujuan\",\n",
    "    when(col(\"status_persetujuan\").isin(\"disetujui\"), \"approved\")\n",
    "    .when(col(\"status_persetujuan\").isin(\"ditolak\"), \"rejected\")\n",
    "    .when(col(\"status_persetujuan\").isin(\"dalam proses\"), \"pending\")\n",
    "    .otherwise(col(\"status_persetujuan\"))\n",
    ")\n",
    "# distinct_status_df = loans_df.select(\"status_persetujuan\").distinct()\n",
    "# display(distinct_status_df)\n",
    "\n",
    "\n",
    "# convert jumlah_pinjaman to fully numerical column. Remove non-numeric string if needed\n",
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "# Remove non-numeric characters except dot and comma, then convert to float\n",
    "loans_df = loans_df.withColumn(\n",
    "    \"jumlah_pinjaman\",\n",
    "    regexp_replace(col(\"jumlah_pinjaman\"), \"[^0-9]\", \"\")\n",
    "    .cast(\"int\")\n",
    ")\n",
    "silver_loans_df = loans_df\n",
    "silver_loans_df.limit(5).display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c89595ee-0acb-4996-bdc2-9da431663dcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Clean Repayment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3dc75b7-3172-4977-b772-4dc806ee7198",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clean repayment data\n",
    "\n",
    "repayments_df = spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.raw_repayments\")\n",
    "repayments_df.limit(5).display()\n",
    "\n",
    "# trim whitespace in tujuan_pinjaman column\n",
    "from pyspark.sql.functions import trim\n",
    "\n",
    "# repayments_df = repayments_df.withColumn(\"tujuan_pinjaman\", trim(\"tujuan_pinjaman\"))\n",
    "\n",
    "# lower case status_persetujuan column\n",
    "from pyspark.sql.functions import lower\n",
    "\n",
    "repayments_df = repayments_df.withColumn(\"status_pembayaran\", lower(\"status_pembayaran\"))\n",
    "# distinct_status_df = repayments_df.select(\"status_pembayaran\").distinct()\n",
    "# display(distinct_status_df)\n",
    "\n",
    "# In column status_pembayaran, convert dsetujui to approved, ditolak to rejected, 'dalam proses' to pending\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "repayments_df = repayments_df.withColumn(\n",
    "    \"status_pembayaran\",\n",
    "    when(col(\"status_pembayaran\").isin(\"tepat waktu\"), \"on time\")\n",
    "    .when(col(\"status_pembayaran\").isin(\"gagal bayar\"), \"default\")\n",
    "    .when(col(\"status_pembayaran\").isin(\"terlambat\"), \"late\")\n",
    "    .otherwise(col(\"status_pembayaran\"))\n",
    ")\n",
    "\n",
    "# distinct_status_df = repayments_df.select(\"status_pembayaran\").distinct()\n",
    "# display(distinct_status_df)\n",
    "\n",
    "\n",
    "# convert jumlah_pinjaman to fully numerical column. Remove non-numeric string if needed\n",
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "# Remove non-numeric characters except dot and comma, then convert to float\n",
    "repayments_df = repayments_df.withColumn(\n",
    "    \"jumlah_angsuran\",\n",
    "    regexp_replace(col(\"jumlah_angsuran\"), \"[^0-9]\", \"\")\n",
    "    .cast(\"int\")\n",
    ")\n",
    "\n",
    "repayments_df = repayments_df.withColumn(\n",
    "    \"jumlah_dibayar\",\n",
    "    regexp_replace(col(\"jumlah_dibayar\"), \"[^0-9]\", \"\")\n",
    "    .cast(\"int\")\n",
    ")\n",
    "\n",
    "silver_repayments_df = repayments_df\n",
    "silver_repayments_df.limit(5).display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80f13c1e-7e02-42b6-b79a-254bcb98463c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_applicants_df.write.mode(\"overwrite\").saveAsTable(f\"{CATALOG}.{SILVER_SCHEMA}.applicants\")\n",
    "silver_loans_df.write.mode(\"overwrite\").saveAsTable(f\"{CATALOG}.{SILVER_SCHEMA}.loans\")\n",
    "silver_repayments_df.write.mode(\"overwrite\").saveAsTable(f\"{CATALOG}.{SILVER_SCHEMA}.repayments\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02-Data-Cleaning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
